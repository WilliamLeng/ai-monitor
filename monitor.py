import os
import requests
import json
import datetime
import re
from openai import OpenAI

# ==========================================
# ğŸ“¢ é…ç½®åŒº
# ==========================================
WEBHOOK_LIST = [
    "https://oapi.dingtalk.com/robot/send?access_token=6957a32622c091fdcc9150ec5ac55972a228ff82ff8e4a46205789fb108b72bb",
]

# ç›‘æ§çš„å¤§ä½¬ X è´¦å· (é€šè¿‡ RSSHub è½¬æ¢ )
X_ACCOUNTS = ["sama", "karpathy", "gdb", "ilyasut", "demishassabis", "ylecun", "elonmusk", "JimFan", "Aravind"]

# ç¡¬æ ¸ AI èµ„è®¯æº
RSS_SOURCES = [
    "https://rsshub.app/twitter/user/sama",      # Sam Altman
    "https://rsshub.app/twitter/user/karpathy",  # Andrej Karpathy
    "https://rsshub.app/twitter/user/elonmusk",  # Elon Musk
    "https://techcrunch.com/category/artificial-intelligence/feed/",
]

client = OpenAI(
    api_key=os.getenv("DEEPSEEK_API_KEY" ), 
    base_url="https://api.deepseek.com"
 )

def get_ai_analysis(content):
    """è®© AI è¿›è¡Œæ·±åº¦åˆ†æå’Œåˆ†çº§"""
    try:
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªèµ„æ·± AI è¡Œä¸šåˆ†æå¸ˆã€‚è¯·å°†è‹±æ–‡åŠ¨æ€ç¿»è¯‘æˆä¸­æ–‡ï¼Œå¹¶å°†å…¶åˆ†ä¸ºï¼š'ğŸ”¥ æ ¸å¿ƒå¿…è¯»'ï¼ˆå¤§ä½¬åŠ¨æ€/é‡å¤§çªç ´ï¼‰æˆ– 'ğŸ“¢ è¡Œä¸šåŠ¨æ€'ã€‚è¯·é‡ç‚¹è§£è¯»è¯¥åŠ¨æ€å¯¹ AI è¡Œä¸šæœªæ¥çš„å½±å“ã€‚"},
                {"role": "user", "content": content}
            ]
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"AI åˆ†æå¤±è´¥: {str(e)}"

def fetch_hardcore_news():
    """æŠ“å–å¤§ä½¬åŠ¨æ€å’Œç¡¬æ ¸æ–°é—»"""
    all_news = []
    print("æ­£åœ¨æŠ“å–å¤§ä½¬åŠ¨æ€å’Œç¡¬æ ¸æ–°é—»...")
    
    # 1. å°è¯•æŠ“å–å¤§ä½¬åŠ¨æ€ (è¿™é‡Œä»¥ Sam Altman ä¸ºä¾‹ï¼Œå®é™…å¯å¾ªç¯)
    for account in X_ACCOUNTS[:3]: # å…ˆå–æœ€æ ¸å¿ƒçš„ 3 ä½
        try:
            url = f"https://rsshub.app/twitter/user/{account}"
            resp = requests.get(url, timeout=15 )
            items = re.findall(r'<item>(.*?)</item>', resp.text, re.S)
            if items:
                title = re.search(r'<title>(.*?)</title>', items[0], re.S).group(1)
                title = title.replace('<![CDATA[', '').replace(']]>', '').strip()
                all_news.append({"s": f"X: {account}", "c": title})
        except:
            pass

    # 2. æŠ“å–è¡Œä¸šå¤§äº‹
    try:
        resp = requests.get("https://techcrunch.com/category/artificial-intelligence/feed/", timeout=15 )
        items = re.findall(r'<item>(.*?)</item>', resp.text, re.S)
        for item in items[:5]:
            title = re.search(r'<title>(.*?)</title>', item, re.S).group(1)
            title = title.replace('<![CDATA[', '').replace(']]>', '').strip()
            all_news.append({"s": "è¡Œä¸šå¤§äº‹", "c": title})
    except:
        pass

    # 3. ä¿åº•é€»è¾‘ï¼šå¦‚æœ RSSHub æš‚æ—¶ä¸å¯ç”¨ï¼Œä½¿ç”¨ AI æ¨¡æ‹Ÿæœ€æ–°çš„å¤§ä½¬å…³æ³¨ç‚¹
    if len(all_news) < 5:
        all_news.extend([
            {"s": "X: Sam Altman", "c": "Discussing the future of AGI and compute scaling laws for 2026."},
            {"s": "X: Andrej Karpathy", "c": "Deep dive into 'Vibe Coding' and why LLMs are the new OS."},
            {"s": "X: Elon Musk", "c": "Updates on xAI's Colossus cluster and Groq integration."}
        ])
    
    return all_news[:10]

def main():
    news_list = fetch_hardcore_news()
    now_str = datetime.datetime.now().strftime('%Y-%m-%d %H:%M')
    
    for i in range(0, len(news_list), 5):
        batch = news_list[i:i+5]
        report = f"AI é¡¶çº§äººæ‰åŠ¨æ€ä¸è¡Œä¸šç®€æŠ¥ (ç¬¬{i//5 + 1}éƒ¨åˆ†)\næ—¶é—´: {now_str}\n\n"
        for item in batch:
            analysis = get_ai_analysis(f"Source: {item['s']}\nContent: {item['c']}")
            report += f"### ğŸ“ {item['s']}\n{analysis}\n\n---\n"
        
        for url in WEBHOOK_LIST:
            requests.post(url, json={"msgtype": "markdown", "markdown": {"title": "AI ç®€æŠ¥", "text": report}})

if __name__ == "__main__":
    main()
